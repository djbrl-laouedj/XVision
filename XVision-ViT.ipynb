{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80c3069",
   "metadata": {},
   "source": [
    "# Documentation général : Génération d’embeddings Sentinel-2 avec CROMA-Large (TorchGeo)\n",
    "\n",
    "Ce document décrit de manière détaillée, claire et structurée le pipeline permettant de générer des embeddings multispectraux à partir d’images Sentinel-2 en utilisant le modèle CROMA-Large de TorchGeo.  \n",
    "Toutes les étapes sont expliquées, avec justification des choix techniques et références vers les sources officielles.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Présentation du modèle CROMA\n",
    "\n",
    "CROMA (Cross-modal Remote Sensing Autoencoder) est un modèle de vision automatique spécialement conçu pour l’observation de la Terre.  \n",
    "Il a été introduit dans l’article suivant :\n",
    "\n",
    "- CROMA — NeurIPS 2024  \n",
    "  https://arxiv.org/abs/2311.00566  \n",
    "- Code officiel  \n",
    "  https://github.com/antofuller/CROMA  \n",
    "- Implémentation TorchGeo  \n",
    "  https://torchgeo.readthedocs.io/en/stable/api/models.html#torchgeo.models.croma_large\n",
    "\n",
    "### Caractéristiques principales\n",
    "\n",
    "* Pré-entraînement auto-supervisé sur **1 million** d’échantillons Sentinel-1 + Sentinel-2 (dataset SSL4EO-S12).\n",
    "* L’encodeur optique utilise **12 bandes Sentinel-2** (la bande cirrus B10 est exclue dans le papier comme dans TorchGeo).\n",
    "* Le modèle utilise une architecture **Vision Transformer (ViT)** adaptée aux données multispectrales.\n",
    "* Taille d’entrée : **120 × 120 pixels** (dimension imposée par l’architecture patch-based).\n",
    "* L’encodeur produit :\n",
    "  - un ensemble d'encodings par patch,\n",
    "  - un embedding global pré-agrégé : **optical_GAP**, utilisé comme représentation finale.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Objectif du script\n",
    "\n",
    "Le script suivant :\n",
    "\n",
    "1. charge un ensemble d’images Sentinel-2 en GeoTIFF,\n",
    "2. extrait et sélectionne les bandes attendues par CROMA (12 bandes),\n",
    "3. normalise chaque bande individuellement,\n",
    "4. redimensionne les images pour correspondre à l’entrée du modèle (120×120),\n",
    "5. applique l’encodeur optique de CROMA-Large (pré-entraîné),\n",
    "6. récupère l’embedding global **optical_GAP**,\n",
    "7. applique une couche de projection optionnelle vers une dimension plus compacte (512),\n",
    "8. sauvegarde tous les embeddings dans un fichier `.npy`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748cfec",
   "metadata": {},
   "source": [
    "# Documentation technique : Génération d’embeddings Sentinel-2 avec CROMA-Large (TorchGeo)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce document détaille le fonctionnement du pipeline permettant de générer des embeddings multispectraux à partir d’images Sentinel-2 en utilisant le modèle CROMA-Large fourni par la bibliothèque TorchGeo.  \n",
    "L’objectif est de produire une représentation vectorielle compacte et informative d’images multispectrales pour des applications en télédétection, classification, retrieval ou intégration dans un futur modèle de langage multimodal.\n",
    "\n",
    "Le modèle CROMA-Large utilisé ici est l’implémentation officielle proposée dans TorchGeo, s’appuyant sur les poids du dépôt original CROMA. Il s’agit d’un encodeur optique pré-entraîné sur de larges volumes de données Sentinel-2.\n",
    "\n",
    "Sources officielles :  \n",
    "- CROMA (NeurIPS 2024) : https://arxiv.org/abs/2311.00566  \n",
    "- Code officiel : https://github.com/antofuller/CROMA  \n",
    "- TorchGeo CROMA-Large : https://torchgeo.readthedocs.io/en/stable/api/models.html  \n",
    "- Dataset SSL4EO-S12 \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Données en entrée (Sentinel-2, 13 bandes)\n",
    "\n",
    "Les images Sentinel-2 L2A sont fournies en 13 bandes :  \n",
    "B1 à B12 + la bande cirrus B10.  \n",
    "Elles sont lues depuis des fichiers GeoTIFF.  \n",
    "Les valeurs sont divisées par 10000 afin de ramener la réflectance sur l’intervalle [0, 1], conformément aux recommandations ESA pour les produits Sentinel-2 L2A.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Sélection des 12 bandes utilisées par CROMA\n",
    "\n",
    "Comme précisé dans l’article original, CROMA ne conserve pas la bande B10 (cirrus).  \n",
    "Cette bande est très bruitée et n’est pas utilisée dans SSL4EO-S12.  \n",
    "TorchGeo suit strictement cette configuration.\n",
    "\n",
    "Les 12 bandes conservées sont donc :  \n",
    "B1 à B9 (sauf B10) puis B11 et B12.\n",
    "\n",
    "Cela garantit la compatibilité parfaite avec le modèle pré-entraîné.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Normalisation par bande\n",
    "\n",
    "Chaque bande est normalisée de manière indépendante selon :\n",
    "\n",
    "- moyenne sur l’ensemble du dataset,\n",
    "- écart-type sur l’ensemble du dataset.\n",
    "\n",
    "La normalisation per-band est indispensable pour stabiliser les représentations multispectrales, réduire les différences dynamiques entre bandes et permettre une convergence cohérente des blocs Transformer.  \n",
    "Elle suit les pratiques classiques en traitement Sentinel-2.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Redimensionnement à 120 × 120\n",
    "\n",
    "CROMA a été pré-entraîné sur des patches Sentinel-2 recadrés spécifiquement en **120 × 120 pixels**, ce qui correspond à sa géométrie interne (patch size = 8 → 15 × 15 patches).  \n",
    "TorchGeo impose la même contrainte.\n",
    "\n",
    "L’interpolation bilinéaire est utilisée avec :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8aceb",
   "metadata": {},
   "source": [
    "align_corners = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ae0ea",
   "metadata": {},
   "source": [
    "Ce paramètre garantit que les distances entre pixels sont conservées lors du redimensionnement, évitant les distorsions spatiales en bordure. C’est le paramètre recommandé par PyTorch pour les architectures modernes (ViT, MAE, CROMA).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Chargement du modèle CROMA-Large (optical-only)\n",
    "\n",
    "Le modèle est initialisé par :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8cd894",
   "metadata": {},
   "source": [
    "croma_large(modalities=[“optical”])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb0567",
   "metadata": {},
   "source": [
    "Cela désactive l’encodeur radar et ne garde que l’encodeur optique Sentinel-2.\n",
    "\n",
    "CROMA-Large utilise un Vision Transformer de dimension d’encodage :\n",
    "\n",
    "- encoder_dim = 1024\n",
    "\n",
    "Le modèle renvoie un dictionnaire contenant plusieurs sorties, dont :\n",
    "\n",
    "- optical_encodings : encodings par patch\n",
    "- optical_GAP : embedding global (Global Average Pooling)\n",
    "\n",
    "Le vecteur optical_GAP est la représentation la plus pertinente pour un pipeline d’embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Utilisation de optical_GAP comme embedding multispectral global\n",
    "\n",
    "La sortie optical_GAP est un vecteur de dimension 1024.  \n",
    "Il s’agit du pooling global des représentations par patch.  \n",
    "Cette représentation a été explicitement conçue dans le papier CROMA pour être :\n",
    "\n",
    "- robuste à la variation géographique,  \n",
    "- robuste à la saisonnalité,  \n",
    "- représentative du contenu spectral et spatial,  \n",
    "- stable pour des tâches downstream (classification, segmentation, retrieval).\n",
    "\n",
    "Dans le papier CROMA, ce vecteur est évalué sur divers benchmarks tels que :  \n",
    "- BigEarthNet-S2,  \n",
    "- BigEarthNet-MM,  \n",
    "- DynamicWorld,  \n",
    "- DFC2020,  \n",
    "montrant qu’il surpasse les méthodes multispectrales précédentes.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Couche de projection (1024 → 512)\n",
    "\n",
    "Une couche linéaire est appliquée afin de réduire la dimension de 1024 à 512.  \n",
    "Cette réduction présente plusieurs avantages :\n",
    "\n",
    "- réduction mémoire lors du stockage des embeddings,  \n",
    "- amélioration de la vitesse dans les bases vectorielles (ex. FAISS),  \n",
    "- standardisation vers une dimension courante pour des modèles multimodaux.\n",
    "\n",
    "Cette étape n’altère pas l’information essentielle, car la rupture est linéaire et le modèle CROMA produit des embeddings redondants permettant une compression modérée.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Génération des embeddings\n",
    "\n",
    "Le pipeline génère les embeddings en batchs pour optimiser l’utilisation du GPU.  \n",
    "Pour chaque batch :\n",
    "\n",
    "1. L’image est passée dans CROMA (optical-only).\n",
    "2. La sortie optical_GAP est extraite.\n",
    "3. L’embedding est projeté en 512 dimensions.\n",
    "4. L’embedding est ajouté à une liste.\n",
    "\n",
    "Les embeddings finaux sont concaténés et sauvegardés dans un fichier `.npy`.\n",
    "\n",
    "Ce format est idéal pour une intégration ultérieure dans un pipeline de machine learning, un moteur de similarité d’images, ou un module de retrieval destiné à un LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Utilisation des embeddings dans un futur LLM\n",
    "\n",
    "Les embeddings issus de CROMA-Large sont adaptés pour :\n",
    "\n",
    "- représenter de manière compacte et fidèle le contenu multispectral Sentinel-2,\n",
    "- permettre la recherche de parcelles similaires,\n",
    "- servir de features dans un modèle supervisé (classification, régression agronomique),\n",
    "- être intégrés dans un pipeline RAG géospatial pour un LLM.\n",
    "\n",
    "CROMA n’est pas un modèle vision-langage, mais il est parfaitement adapté pour fournir au LLM des informations d’état de la parcelle.  \n",
    "L’intégration se fait typiquement via un espace vectoriel FAISS ou un adapter linéaire.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Ce pipeline exploite fidèlement la version officielle du modèle CROMA-Large proposée dans TorchGeo, en respectant :\n",
    "\n",
    "- les bandes Sentinel-2 utilisées lors du pré-entraînement,  \n",
    "- la normalisation multispectrale,  \n",
    "- les dimensions d’entrée requises,  \n",
    "- les sorties officielles du modèle,  \n",
    "- la préparation d’un embedding propre pour un pipeline IA moderne.\n",
    "\n",
    "Il fournit ainsi une base solide pour la création d’un assistant agricole, d’un moteur de recommandation, ou d’un système de retrieval géospatial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchgeo.models import croma_large\n",
    "\n",
    "# ============ DEVICE ============\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 1) Load Sentinel-2 images\n",
    "# ===============================\n",
    "paths = glob.glob(\"AnnualCrop/*.tif\")\n",
    "\n",
    "images = []\n",
    "for p in paths:\n",
    "    with rasterio.open(p) as src:\n",
    "        x = src.read().astype(np.float32) / 10000.0   # (13,H,W)\n",
    "        images.append(x)\n",
    "\n",
    "X = np.array(images, dtype=np.float32)   # (N,13,H,W)\n",
    "\n",
    "# ===============================\n",
    "# 2) KEEP 12 BANDS EXPECTED BY CROMA\n",
    "# Remove B10 (index 9)\n",
    "# ===============================\n",
    "keep_idx = list(range(9)) + [10, 11, 12]\n",
    "X = X[:, keep_idx, :, :]      # (N,12,H,W)\n",
    "\n",
    "# ===============================\n",
    "# 3) Normalization PER BAND\n",
    "# ===============================\n",
    "mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "std  = X.std(axis=(0, 2, 3), keepdims=True)\n",
    "X = (X - mean) / (std + 1e-9)\n",
    "\n",
    "# ===============================\n",
    "# 4) Resize → 120×120 expected by CROMA\n",
    "# ===============================\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "X_tensor = F.interpolate(X_tensor, size=(120, 120), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# ===============================\n",
    "# 5) Load CROMA-Large pretrained (OPTICAL ONLY)\n",
    "# ===============================\n",
    "model = croma_large(modalities=[\"optical\"]).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===============================\n",
    "# 7) Compute embeddings\n",
    "# ===============================\n",
    "batch_size = 16\n",
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        batch = X_tensor[i:i+batch_size].to(device)\n",
    "\n",
    "        # ---- OUTPUT = dict with \"optical_GAP\" key ----\n",
    "        output = model(x_optical=batch)\n",
    "        feats = output[\"optical_GAP\"]       # correct key\n",
    "\n",
    "        embeddings.append(feats.cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "np.save(\"sentinel_embeddings_1024.npy\", embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
