{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RÉPONSE DU MODÈLE =====\n",
      "\n",
      "L’analyse spectrale montre une végétation modérée et cultures en croissance active. L’indice d’humidité suggère couvert végétal bien hydraté. Selon le SAVI, mélange sol + végétation. L’EVI indique biomasse élevée. L’ARVI montre végétation relativement stable malgré l'atmosphère. Le DVI confirme activité végétale moyenne. Le ATVI indique végétation relativement stable malgré l'atmosphère.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import numpy as np\n",
    "\n",
    "# =====================================\n",
    "# 1) DEVICE\n",
    "# =====================================\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# =====================================\n",
    "# 2) CHARGER TOKENIZER\n",
    "# =====================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"qwen2_lora_multimodal\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# =====================================\n",
    "# 3) CHARGER LE MODÈLE DE BASE\n",
    "# =====================================\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-1.5B\",\n",
    "    dtype=torch.float16,          # pour transformers >=4.57\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# =====================================\n",
    "# 4) CHARGER LoRA FINETUNÉ\n",
    "# =====================================\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"qwen2_lora_multimodal\"\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# =====================================\n",
    "# 5) VisionAdapter (doit matcher EXACTEMENT celui du training)\n",
    "# =====================================\n",
    "class VisionAdapter(torch.nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden=4096,\n",
    "                 llm_dim=base_model.config.hidden_size,\n",
    "                 num_tokens=64):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.llm_dim = llm_dim\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, num_tokens * llm_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)  # (B, num_tokens * hidden)\n",
    "        return out.view(x.size(0), self.num_tokens, self.llm_dim)\n",
    "\n",
    "# Charger l’adapter\n",
    "adapter = VisionAdapter().to(device)\n",
    "adapter.load_state_dict(torch.load(\"vision_adapter.pt\", map_location=device))\n",
    "adapter.eval()\n",
    "\n",
    "# =====================================\n",
    "# 6) Fonction pour injecter les tokens visuels (INFÉRENCE)\n",
    "# =====================================\n",
    "def inject_visual_tokens_for_inference(adapter, embed, model, tokenizer):\n",
    "    # embed: (1, 1024)\n",
    "    with torch.no_grad():\n",
    "        vis_tokens = adapter(embed)                    # (1, 64, hidden)\n",
    "\n",
    "    # Choisir un token de départ : bos si dispo, sinon eos, sinon pad\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    if bos_id is None:\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            bos_id = tokenizer.eos_token_id\n",
    "        else:\n",
    "            bos_id = tokenizer.pad_token_id\n",
    "\n",
    "    dummy = torch.tensor([[bos_id]], device=device)\n",
    "\n",
    "    text_embedding_layer = model.get_input_embeddings()\n",
    "    dummy_text_emb = text_embedding_layer(dummy)       # (1, 1, hidden)\n",
    "\n",
    "    # aligner dtype (LLM en float16)\n",
    "    vis_tokens = vis_tokens.to(dtype=dummy_text_emb.dtype)\n",
    "\n",
    "    # concaténation visuels + dummy\n",
    "    full = torch.cat([vis_tokens, dummy_text_emb], dim=1)  # (1, 64+1, hidden)\n",
    "\n",
    "    # mask : 1 sur tout\n",
    "    mask_vis = torch.ones((1, vis_tokens.size(1)), device=device, dtype=torch.long)\n",
    "    mask_dummy = torch.ones((1, 1), device=device, dtype=torch.long)\n",
    "    full_mask = torch.cat([mask_vis, mask_dummy], dim=1)   # (1, 64+1)\n",
    "\n",
    "    return full, full_mask\n",
    "\n",
    "# =====================================\n",
    "# 7) Charger un embedding (ex: index 0)\n",
    "# =====================================\n",
    "embeddings = np.load(\"sentinel_embeddings_1024.npy\")   # (N, 1024)\n",
    "embed = torch.tensor(embeddings[0], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# =====================================\n",
    "# 8) Préparer le prompt texte\n",
    "# =====================================\n",
    "prompt = (\n",
    "            \"Analyse l’état agricole de la parcelle à partir de l’embedding visuel.\\n\"\n",
    "            \"Décris en quelques phrases :\\n\"\n",
    "            \"- la vigueur de la végétation\\n\"\n",
    "            \"- l’humidité du couvert\\n\"\n",
    "            \"- la proportion sol/végétation\\n\"\n",
    "            \"- la biomasse\\n\"\n",
    "            \"- l’état général de la croissance\\n\"\n",
    "        )\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "text_embedding_layer = model.get_input_embeddings()\n",
    "text_embeds = text_embedding_layer(input_ids)         # (1, L, hidden)\n",
    "\n",
    "# =====================================\n",
    "# 9) Injecter les tokens visuels\n",
    "# =====================================\n",
    "vis_embeds, vis_mask = inject_visual_tokens_for_inference(adapter, embed, model, tokenizer)\n",
    "\n",
    "# concat visuels + texte\n",
    "inputs_embeds = torch.cat([vis_embeds, text_embeds], dim=1)  # (1, 64+L, hidden)\n",
    "\n",
    "# construire le mask complet\n",
    "mask_text = torch.ones_like(input_ids, dtype=torch.long)\n",
    "attention_mask = torch.cat([vis_mask, mask_text], dim=1)     # (1, 64+L)\n",
    "\n",
    "# =====================================\n",
    "# 10) GÉNÉRATION\n",
    "# =====================================\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "# =====================================\n",
    "# 11) AFFICHAGE RÉSULTAT\n",
    "# =====================================\n",
    "text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\n===== RÉPONSE DU MODÈLE =====\\n\")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
